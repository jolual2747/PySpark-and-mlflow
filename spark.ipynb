{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages  org.apache.spark:spark-avro_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Spark_ML\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('delta')\\\n",
    "    .load('delta/saber11')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/05 00:26:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10361"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lenguaje: string (nullable = true)\n",
      " |-- matematicas: string (nullable = true)\n",
      " |-- sociales: string (nullable = true)\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- filosofia: string (nullable = true)\n",
      " |-- biologia: string (nullable = true)\n",
      " |-- quimica: string (nullable = true)\n",
      " |-- fisica: string (nullable = true)\n",
      " |-- nivel_ingles: string (nullable = true)\n",
      " |-- sisben: integer (nullable = true)\n",
      " |-- estrato: integer (nullable = true)\n",
      " |-- genero: string (nullable = true)\n",
      " |-- puntaje_saber11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values at column lenguaje\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|lenguaje|\n",
      "+--------+\n",
      "|    bajo|\n",
      "|   medio|\n",
      "|superior|\n",
      "|    alto|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Unique values at column matematicas\n",
      "\n",
      "+-----------+\n",
      "|matematicas|\n",
      "+-----------+\n",
      "|       bajo|\n",
      "|      medio|\n",
      "|   superior|\n",
      "|       alto|\n",
      "+-----------+\n",
      "\n",
      "\n",
      "Unique values at column sociales\n",
      "\n",
      "+--------+\n",
      "|sociales|\n",
      "+--------+\n",
      "|    bajo|\n",
      "|   medio|\n",
      "|superior|\n",
      "|    alto|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Unique values at column filosofia\n",
      "\n",
      "+---------+\n",
      "|filosofia|\n",
      "+---------+\n",
      "|     bajo|\n",
      "|    medio|\n",
      "| superior|\n",
      "|     alto|\n",
      "+---------+\n",
      "\n",
      "\n",
      "Unique values at column biologia\n",
      "\n",
      "+--------+\n",
      "|biologia|\n",
      "+--------+\n",
      "|    bajo|\n",
      "|   medio|\n",
      "|superior|\n",
      "|    alto|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Unique values at column quimica\n",
      "\n",
      "+--------+\n",
      "| quimica|\n",
      "+--------+\n",
      "|    bajo|\n",
      "|   medio|\n",
      "|superior|\n",
      "|    alto|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Unique values at column fisica\n",
      "\n",
      "+--------+\n",
      "|  fisica|\n",
      "+--------+\n",
      "|    bajo|\n",
      "|superior|\n",
      "|   medio|\n",
      "|    alto|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Unique values at column nivel_ingles\n",
      "\n",
      "+------------+\n",
      "|nivel_ingles|\n",
      "+------------+\n",
      "|          A2|\n",
      "|          A-|\n",
      "|          B1|\n",
      "|          B+|\n",
      "|          A1|\n",
      "+------------+\n",
      "\n",
      "\n",
      "Unique values at column sisben\n",
      "\n",
      "+------+\n",
      "|sisben|\n",
      "+------+\n",
      "|     1|\n",
      "|     3|\n",
      "|    -9|\n",
      "|     5|\n",
      "|     4|\n",
      "|     2|\n",
      "+------+\n",
      "\n",
      "\n",
      "Unique values at column estrato\n",
      "\n",
      "+-------+\n",
      "|estrato|\n",
      "+-------+\n",
      "|      1|\n",
      "|      6|\n",
      "|      3|\n",
      "|      5|\n",
      "|      4|\n",
      "|      2|\n",
      "+-------+\n",
      "\n",
      "\n",
      "Unique values at column genero\n",
      "\n",
      "+------+\n",
      "|genero|\n",
      "+------+\n",
      "|Hombre|\n",
      "| Mujer|\n",
      "+------+\n",
      "\n",
      "\n",
      "Unique values at column puntaje_saber11\n",
      "\n",
      "+---------------+\n",
      "|puntaje_saber11|\n",
      "+---------------+\n",
      "|           bajo|\n",
      "|          medio|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    if column != 'Id':\n",
    "        print(f'\\nUnique values at column {column}\\n')\n",
    "        df.select(column).distinct().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('id')\n",
    "df = df.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target to binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('puntaje_saber11',\n",
    "                   when(col('puntaje_saber11')=='bajo', 0)\\\n",
    "                   .otherwise(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String Indexer, OneHotEncoder and MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features for OHE and MinMaxScaler\n",
    "\n",
    "OHE_cols = [x[0] for x in df.dtypes if x[1] == 'string' and x[0] != 'puntaje_saber11']\n",
    "to_scale = [x[0] for x in df.dtypes if x[1] in ['int', 'float'] and x[0] != 'puntaje_saber11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+--------------------------+-----------------------+------------------------+-----------------------+----------------------+---------------------+---------------------------+---------------------+--------------------+--------------------+\n",
      "|puntaje_saber11|lenguaje_encoded_scaled|matematicas_encoded_scaled|sociales_encoded_scaled|filosofia_encoded_scaled|biologia_encoded_scaled|quimica_encoded_scaled|fisica_encoded_scaled|nivel_ingles_encoded_scaled|genero_encoded_scaled|       sisben_scaled|      estrato_scaled|\n",
      "+---------------+-----------------------+--------------------------+-----------------------+------------------------+-----------------------+----------------------+---------------------+---------------------------+---------------------+--------------------+--------------------+\n",
      "|              0|          [1.0,0.0,0.0]|             [1.0,0.0,0.0]|          [1.0,0.0,0.0]|           [1.0,0.0,0.0]|          [0.0,1.0,0.0]|         [0.0,1.0,0.0]|        [0.0,0.0,1.0]|              (4,[0],[1.0])|                [1.0]|[0.7857142857142857]|               [0.4]|\n",
      "|              0|          [0.0,1.0,0.0]|                 (3,[],[])|          [0.0,1.0,0.0]|           [1.0,0.0,0.0]|          [0.0,1.0,0.0]|         [1.0,0.0,0.0]|        [1.0,0.0,0.0]|              (4,[0],[1.0])|                [1.0]|[0.7142857142857142]|               [0.0]|\n",
      "|              0|          [0.0,1.0,0.0]|             [0.0,1.0,0.0]|          [0.0,1.0,0.0]|           [0.0,1.0,0.0]|          [0.0,1.0,0.0]|         [0.0,1.0,0.0]|        [0.0,1.0,0.0]|              (4,[0],[1.0])|                [0.0]|               [1.0]|               [0.4]|\n",
      "|              0|          [0.0,1.0,0.0]|             [0.0,1.0,0.0]|          [0.0,1.0,0.0]|           [1.0,0.0,0.0]|          [0.0,1.0,0.0]|         [1.0,0.0,0.0]|        [1.0,0.0,0.0]|              (4,[2],[1.0])|                [1.0]|               [1.0]|[0.6000000000000001]|\n",
      "|              0|          [0.0,1.0,0.0]|             [0.0,1.0,0.0]|          [0.0,1.0,0.0]|           [1.0,0.0,0.0]|          [0.0,1.0,0.0]|         [0.0,1.0,0.0]|        [1.0,0.0,0.0]|              (4,[0],[1.0])|                [1.0]|[0.7142857142857142]|               [0.0]|\n",
      "+---------------+-----------------------+--------------------------+-----------------------+------------------------+-----------------------+----------------------+---------------------+---------------------------+---------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple Indexers for every column in OHE_cols\n",
    "indexer = [StringIndexer(inputCol = col, outputCol = col+'_indexed') for col in OHE_cols]\n",
    "\n",
    "# Multiple OHE for every column in OHE_cols\n",
    "encoder = [OneHotEncoder(inputCol = col+'_indexed', outputCol = col+'_encoded') for col in OHE_cols]\n",
    "\n",
    "aux_features = [col+'_encoded' for col in OHE_cols] + to_scale\n",
    "\n",
    "# Vector Assembler to can apply MinMaxScaler\n",
    "vector_to_scale = [VectorAssembler(inputCols = [col], outputCol = col+'_vec') for col in aux_features]\n",
    "scaler = [MinMaxScaler(inputCol = col+'_vec', outputCol = col+'_scaled') for col in aux_features]\n",
    "\n",
    "# Final Vector Assembler\n",
    "vector = VectorAssembler(inputCols = [col+'_scaled' for col in aux_features], outputCol ='features')\n",
    "\n",
    "transform_steps = indexer + encoder + vector_to_scale + scaler\n",
    "\n",
    "# Pipeline\n",
    "features_pipeline = Pipeline(stages = transform_steps)\n",
    "df_trans = features_pipeline.fit(df).transform(df)\n",
    "to_select = [col for col in df_trans.columns if '_scaled' in col or col == 'puntaje_saber11']\n",
    "df_trans.select(to_select).show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline (logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# to_vector = to_select\n",
    "# to_vector.append('puntaje_saber11')\n",
    "model_lr = LogisticRegression(featuresCol = 'features', labelCol = 'puntaje_saber11')\n",
    "pipeline_lr = Pipeline(stages = [features_pipeline, vector, model_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testingData) = df.randomSplit([0.70, 0.3])\n",
    "lr_model = pipeline_lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+----------------------------------------+----------+\n",
      "|features                                                                   |probability                             |prediction|\n",
      "+---------------------------------------------------------------------------+----------------------------------------+----------+\n",
      "|(28,[0,3,6,10,12,15,18,22,25],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |[0.25516373223123756,0.7448362677687624]|1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22,25],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |[0.25516373223123756,0.7448362677687624]|1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22,25],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |[0.25516373223123756,0.7448362677687624]|1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22,25],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |[0.25516373223123756,0.7448362677687624]|1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22,25],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |[0.25516373223123756,0.7448362677687624]|1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |[0.2490534978895824,0.7509465021104176] |1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |[0.2490534978895824,0.7509465021104176] |1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])              |[0.2490534978895824,0.7509465021104176] |1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22,25,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.2])|[0.2774940436960577,0.7225059563039423] |1.0       |\n",
      "|(28,[0,3,6,10,12,15,18,22,25,27],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.2])|[0.2774940436960577,0.7225059563039423] |1.0       |\n",
      "+---------------------------------------------------------------------------+----------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_summary = lr_model.stages[2].summary.predictions\n",
    "lr_summary = lr_summary.select('features', 'probability' ,'prediction')\n",
    "lr_summary.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = lr_model.transform(testingData).withColumnRenamed(\"puntaje_saber11\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6779062299293513"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol = 'prediction', labelCol = 'label')\n",
    "evaluator.evaluate(test,  {evaluator.metricName: \"accuracy\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
